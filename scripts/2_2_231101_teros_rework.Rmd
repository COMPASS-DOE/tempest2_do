---
title: "231101_teros_rework"
output: html_document
date: "2023-11-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      warning = F,
                      message = F)
```

### Purpose

Althought the TEROS data was worked up already in 230710_clean_teros, the resulting dataset still doesn't seem right. I'm going to take the datasets cleaned of obvious errors, but before any merging / binning occurs to see what makes sense to use, and how to process them.


### The problem

Based on the script referenced, here are the "final" datasets for TEROS, which clearly show jumpy behavior indicative of inappropriate binning.

```{r}

require(pacman)

p_load(tidyverse, 
       zoo,
       cowplot)

theme_set(theme_bw())

old_data <- read_csv("../data/old_teros_medians.csv")

old_data %>% 
  ggplot(aes(datetime, vwc)) + 
  geom_line() + 
  facet_wrap(plot~depth)
```
We'll start with the raw datasets, and go from there. Let's start by looking at Control at 30cm since we know what that pattern should be, and there are two clear issues to diagnose: the step change, and the spikes

```{r load raw datasets}
raw_5min <- read_csv("../data/231101_teros_5min_raw.csv")
raw_15min <- read_csv("../data/231101_teros_15min_raw.csv")

raw_vwc_plot <- function(data){
  
  data %>% 
    filter(plot == "Control" & depth == 30) %>% 
    ggplot(aes(datetime, vwc)) + 
    geom_line() + 
    facet_wrap(~grid_square)
}

plot_grid(raw_vwc_plot(raw_5min) + ggtitle("5-minute"), 
          raw_vwc_plot(raw_15min) + ggtitle("15-minute"))
```
If we combine the 15-minute data as means and medians, does that fix things? Nope. But we do see that median values have spikes while mean values are a little smaller. Not sure if that helps, but let's look at the mean first, as a stats front-runner

```{r}
raw_15min %>% 
  filter(plot == "Control" & depth == 30) %>% 
  group_by(datetime) %>% 
  summarize(mean_vwc = mean(vwc, na.rm = T), 
            median_vwc = median(vwc, na.rm = T)) %>% 
  ggplot(aes(datetime)) + 
  geom_line(aes(y = mean_vwc), color = "red") + 
  geom_line(aes(y = median_vwc), color = "blue", alpha = 0.5) + 
  ggtitle("Red: mean, Blue: median")
```

Let's zoom in on the weird issue here, and see if we can parse what's happening: data for Control at 30 cm from 6/6 at noon till 6/7 at noon: colors are individual grid squares, black is mean across all grid squares.


```{r}

x1_start = as_datetime("2023-06-06 12:00:00")

x1 <- raw_15min %>% 
  filter(plot == "Control" & depth == 30) %>% 
  filter(datetime > x1_start & datetime < x1_start + days(1))

x1_mean <- x1 %>% 
  group_by(datetime) %>% 
  summarize(mean_vwc = mean(vwc, na.rm = T))

ggplot(x1, aes(datetime, vwc, color = grid_square)) + 
  geom_line() + 
  geom_line(data = x1_mean, aes(y = mean_vwc), color = "black")
```

OOOOOOOOKKKKKAAAAYYYY. Now we're getting somewhere. We need some sort of filter to remove sensors that aren't persistent through the flood time-period. Let's find some stats for that, then we can filter out some of these troublesome sensors (and see what we're left with).

```{r}
event_coverage <- raw_15min %>% 
  filter(datetime > "2023-06-06" & 
           datetime < "2023-06-10") %>% 
  ungroup() %>% 
  group_by(plot, depth, grid_square) %>% 
  count() 

event_coverage %>% 
  ggplot(aes(grid_square, n, fill = as.factor(depth))) + 
  geom_col(position = "dodge") + 
  facet_wrap(~plot, ncol = 1)

threshold = 350

sensors_to_scrub <-event_coverage %>% 
  filter(n < threshold)
 
join_cols = c("plot", "depth", "grid_square")

clean1_15min <- anti_join(raw_15min, sensors_to_scrub %>% select(all_of(join_cols)), by = join_cols)

clean1_15min
```
```{r}

y1_start = x1_start

y1 <- clean1_15min %>% 
  filter(plot == "Control" & depth == 30) %>% 
  filter(datetime > y1_start & datetime < y1_start + days(1))

y1_mean <- y1 %>% 
  group_by(datetime) %>% 
  summarize(mean_vwc = mean(vwc, na.rm = T))

ggplot(y1, aes(datetime, vwc, color = grid_square)) + 
  geom_line() + 
  geom_line(data = y1_mean, aes(y = mean_vwc), color = "black")
```

We have fixed one problem, but the more pervasive (and maybe more important) one is these random spikes. Let's get into this one:

```{r}

z1_start <- as_datetime("2023-06-06 15:00:00")

z1 <- clean1_15min %>% 
  filter(plot == "Control" & depth == 30) %>% 
  filter(datetime > z1_start & datetime < z1_start + hours(3))

z1_mean <- z1 %>% 
  group_by(datetime) %>% 
  summarize(mean_vwc = mean(vwc, na.rm = T))

ggplot(z1, aes(datetime, vwc, color = grid_square)) + 
  geom_line() + 
  geom_point() + 
  geom_line(data = z1_mean, aes(y = mean_vwc), color = "black")
```

TEROS are starting to piss me off.... so we have data gap issues. I think we'll need to reconstruct a time-series then full_join to ID where our NAs are, then we'll need to fill them. 


```{r}

## First, make a vector of all the datetimes
data_start = min(clean1_15min$datetime)
data_end = max(clean1_15min$datetime)
interval = "15 min"
datetime = seq(from = data_start, 
                to = data_end, 
                by = interval)

## Next, make vector of the grouping variables
grouping_combos <- unique(clean1_15min[c("plot", "depth", "grid_square")])

## Now grid_expand to all theoretical rows
all_possible_rows <- expand_grid(datetime, grouping_combos)

## Calculate how many NAs we expect: 506. Not terrible, not great
nrow(all_possible_rows) - nrow(clean1_15min)
```

```{r join data}

## NOTE: I was getting vector memory exhausted errors, and followed this: 
## https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
## which seems to have fixed it
df1 <- full_join(all_possible_rows, clean1_15min, by = c("datetime", join_cols)) %>% 
  mutate(gap_filled = ifelse(is.na(vwc), TRUE, FALSE))
```


First let's look at how many NAs we have by sensor. Good news: consistently less than 2%, and usually less than 1%
```{r}
df1 %>% 
  group_by(plot, depth, grid_square) %>% 
  summarize(count = n(), 
            na_count = sum(is.na(vwc))) %>% 
  mutate(perc_na = (na_count / count) * 100) %>% 
  ggplot(aes(grid_square, perc_na, fill = as.factor(depth))) + 
  geom_col(position = "dodge") + 
  facet_wrap(~plot, ncol = 1) + 
  ggtitle("% NAs by sensor")
``` 

Next, and maybe more important, let's look at how long the gaps are. The longest gaps are around 10 values, which is 2.5 hours. That's a decently long time to gap-fill if it occurs during the event. 

```{r}
df1 %>% 
  group_by(plot, depth, grid_square) %>% 
  mutate(is_na_vwc = is.na(vwc),
    gap_id = cumsum(is_na_vwc),
    gap_length = ifelse(is_na_vwc, NA, gap_id)) %>% 
  ggplot(aes(grid_square, gap_length, fill = as.factor(depth))) + 
  geom_boxplot(alpha = 0.2) + 
  facet_wrap(~plot, ncol = 1) + 
  ggtitle("NA gap length by sensor")
```

Let's quickly visualize when these gaps are occuring:

```{r}
df1 %>% 
  mutate(vwc = ifelse(is.na(vwc), 0.5, vwc)) %>% 
  ggplot(aes(datetime, vwc, group_by(grid_square), color = as.factor(depth))) + 
  geom_line() + 
  facet_wrap(depth~plot)
```

Ohhhhh ratz. That's a lot, and they're happening during the event... Hmm. First, looking two plots up, we have a TON of sensors at 15 cm, but not a lot with depth resolution, which is the whole point of this dataset. So first, let's scrub any sensor that's only 15cm.

```{r}
sensors_w_multiple_depths <- grouping_combos %>% 
  group_by(plot, grid_square) %>%
  summarize(num_depths = n_distinct(depth)) %>% 
  filter(num_depths == 3) %>% 
  select(-num_depths)
```

```{r}
df2 <- inner_join(df1, sensors_w_multiple_depths, by = c("plot", "grid_square"))

df2 %>% 
  mutate(vwc = ifelse(is.na(vwc), 0.5, vwc)) %>% 
  ggplot(aes(datetime, vwc, color = (grid_square))) + 
  geom_line() + 
  facet_wrap(depth~plot)
```

Okay, we have a lot less data, but we're also seeing what appear to be some consistent patterns. Time to gap-fill.

```{r}

df2_filled <- df2 %>% 
  #select(-contains("_id")) %>% 
  ungroup() %>% 
  group_by(plot, grid_square, depth) %>% 
  mutate(across(where(is.numeric), ~ifelse(is.na(.), zoo::na.approx(., na.rm = FALSE), .), .names = "{.col}_filled")) %>%
  ungroup()

df2_means <- df2_filled %>% 
  group_by(datetime, plot, depth) %>% 
  summarize(across(where(is.numeric), mean))
  
  ggplot(df2_filled %>% filter(gap_filled == FALSE), 
                               aes(datetime, vwc_filled, color = (grid_square))) + 
  geom_line() + 
  geom_point(data = df2_filled %>% filter(gap_filled == TRUE)) + 
  geom_line(data = df2_means, color = "black") + 
  facet_wrap(depth~plot) + 
  ggtitle("Points show gaps filled, black = mean")
  
  
```

### Summarize and export final dataset {.tabset}

We FINALLLY have data that look sensible (black lines above). Let's bin by mean, plot one more time to make sure, then export.

```{r}

df_final <- df2_filled %>% 
  select(-contains("_id")) %>% 
  group_by(datetime, plot, depth) %>% 
  mutate(filled = ifelse(gap_filled == TRUE, 1, 0)) %>% 
  select(contains("filled")) %>%  ## A little tricky with strings here (include "filled")
  summarize(across(where(is.numeric), mean)) %>%
  rename_with(~str_remove(., "_filled"), contains("_filled")) %>% # now exclude "filled"
  mutate(filled = ifelse(filled > 0, TRUE, FALSE))

plot_final_data <- function(var){
  ggplot(df_final, aes(datetime, {{var}}, color = as.factor(depth))) + 
    geom_line() + 
    geom_point(data = df_final %>% filter(filled == TRUE)) + 
    facet_wrap(~plot)
}

write_csv(df_final, "../data/231102_teros_final.csv")
```

#### Temp

```{r}
plot_final_data(tsoil)
```


#### VWC


```{r}
plot_final_data(vwc)
```

#### EC

```{r}
plot_final_data(ec)
```